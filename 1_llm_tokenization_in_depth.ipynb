{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### why do llm's need tokenization?\n",
    "\n",
    "Tokens are the atomic units of text that are used to train transformers.\n",
    "\n",
    "If we take every word as a token, all languages combined have 1.5 million words, , its not possible to train a model with this many words, because of the computational cost. If we take each character as a token, it would be computationally expensive to predict the next token at character level.\n",
    "\n",
    "So, we need to find a way to represent the text in a way that is both efficient and meaningful.\n",
    "\n",
    "**BPE (Byte Pair Encoding)** is a data compression technique that is used in LLMs to tokenize the text. BPE is a greedy algorithm that iteratively merges the most frequent pair of tokens until the desired vocabulary size is reached. BPE is used in GPT-2, GPT-3, and other LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unicode\n",
    "String is a immutable sequence of characters represented in unicode, where each character is a unicode code point. We have 1.11 million unicode code points for all languages combined.Types of unicode encodings: UTF-8, UTF-16, UTF-32. UTF-8 is the most common encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character: a, Unicode code point: 97, UTF-8 values: [97], Number of bytes: 1, Byte representation: 61\n",
      "Character: ¥, Unicode code point: 165, UTF-8 values: [194, 165], Number of bytes: 2, Byte representation: c2a5\n",
      "Character: అ, Unicode code point: 3077, UTF-8 values: [224, 176, 133], Number of bytes: 3, Byte representation: e0b085\n",
      "Character: ह, Unicode code point: 2361, UTF-8 values: [224, 164, 185], Number of bytes: 3, Byte representation: e0a4b9\n",
      "Character: ف, Unicode code point: 1601, UTF-8 values: [217, 129], Number of bytes: 2, Byte representation: d981\n"
     ]
    }
   ],
   "source": [
    "## UTF-8 encoding of different characters with their byte representation and number of bytes.\n",
    "def print_byte_info(char):\n",
    "    byte_info = char.encode('utf-8')\n",
    "    print(f\"Character: {char}, Unicode code point: {ord(char)}, UTF-8 values: {list(byte_info)}, Number of bytes: {len(byte_info)}, Byte representation: {byte_info.hex()}\")\n",
    "\n",
    "\n",
    "print_byte_info('a') \n",
    "print_byte_info('¥') # yen symbol\n",
    "print_byte_info('అ') # Telugu character\n",
    "print_byte_info('ह')  # Hindi character\n",
    "print_byte_info('ف') # Arabic character\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets go through details of BPE implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sudo code for BPE**\n",
    "```\n",
    "step1 : text = \"aabcaabcda\"\n",
    "        - len of text = 9, \n",
    "        - unique tokens = 4, {a,b,c,d} -> token size is 4\n",
    "        - consecutive tokens : (a,a), (a,b), (b,c), (c,a), (a,a), (a,b), (b,c), (c,d), (d,a)\n",
    "        - if we take value count of each pair, we get the following:\n",
    "            {(a,a):2, (a,b):2, (b,c):2, (c,a):1, (a,b):2, (b,c):1, (c,d):1, (d,a):1}\n",
    "step2 : merge the most frequent pair of tokens.\n",
    "        for each iteration, merge the most frequent pair of tokens with new token.\n",
    "        - merge (a,a) with new token 'e'\n",
    "        - Then new text = \"ebcebcda\"\n",
    "        - new unique tokens = 5, {a,b,c,d,e}\n",
    "        - new consecutive tokens : (e,b), (b,c), (c,e), (e,b), (b,c), (c,d), (d,a)\n",
    "        - if we take value count of each pair, we get the following:\n",
    "            {(e,b):2, (b,c):2, (c,e):1, (e,b):2, (b,c):1, (c,d):1, (d,a):1}\n",
    "        - now len of text = 8, \n",
    "        - unique tokens = 5, {a,b,c,d,e} -> token size is 5\n",
    "step3 : repeat step2 until the desired vocabulary size is reached.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try with a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73, 110, 32, 97, 32, 115, 109, 97, 108, 108, 44, 32, 115, 108, 101, 101, 112, 121, 32, 116, 111, 119, 110, 32, 110, 101, 115, 116, 108, 101, 100, 32, 98, 101, 116, 119, 101, 101, 110, 32, 114, 111, 108, 108, 105, 110, 103, 32, 104, 105, 108, 108, 115, 44, 32, 116, 104, 101, 114, 101, 32, 119, 97, 115, 32, 97, 110, 32, 111, 108, 100, 44, 32, 110, 101, 103, 108, 101, 99, 116, 101, 100, 32, 109, 117, 115, 105, 99, 32, 115, 104, 111, 112, 32, 99, 97, 108, 108, 101, 100, 32, 226, 128, 156, 72, 97, 114, 109, 111, 110, 105, 101, 115, 32, 76, 111, 115, 116, 46, 226, 128, 157, 32, 73, 116, 115, 32, 119, 105, 110, 100, 111, 119, 115, 32, 119, 101, 114, 101, 32, 99, 114, 97, 99, 107, 101, 100, 44, 32, 116, 104, 101, 32, 119, 111, 111, 100, 101, 110, 32, 115, 105, 103, 110, 32, 115, 119, 97, 121, 101, 100, 32, 105, 110, 32, 116, 104, 101, 32, 98, 114, 101, 101, 122, 101, 44, 32, 97, 110, 100, 32, 100, 117, 115, 116, 32, 99, 111, 118, 101, 114, 101, 100, 32, 101, 118, 101, 114, 121, 32, 115, 117, 114, 102, 97, 99, 101, 32, 105, 110, 115, 105, 100, 101, 46, 32, 77, 111, 115, 116, 32, 111, 102, 32, 116, 104, 101, 32, 116, 111, 119, 110, 115, 102, 111, 108, 107, 32, 104, 97, 100, 32, 102, 111, 114, 103, 111, 116, 116, 101, 110, 32, 97, 98, 111, 117, 116, 32, 105, 116, 44, 32, 112, 97, 115, 115, 105, 110, 103, 32, 105, 116, 32, 98, 121, 32, 97, 115, 32, 116, 104, 101, 121, 32, 104, 117, 114, 114, 105, 101, 100, 32, 111, 110, 32, 119, 105, 116, 104, 32, 116, 104, 101, 105, 114, 32, 98, 117, 115, 121, 32, 108, 105, 118, 101, 115, 46, 32, 66, 117, 116, 32, 116, 111, 32, 111, 110, 101, 32, 112, 101, 114, 115, 111, 110, 44, 32, 105, 116, 32, 119, 97, 115, 32, 97, 32, 112, 108, 97, 99, 101, 32, 111, 102, 32, 119, 111, 110, 100, 101, 114, 226, 128, 148, 97, 32, 112, 108, 97, 99, 101, 32, 116, 104, 97, 116, 32, 104, 101, 108, 100, 32, 116, 104, 101, 32, 107, 101, 121, 32, 116, 111, 32, 97, 32, 109, 121, 115, 116, 101, 114, 121, 32, 115, 104, 101, 32, 99, 111, 117, 108, 100, 110, 226, 128, 153, 116, 32, 108, 101, 116, 32, 103, 111, 32, 111, 102, 46]\n",
      "438\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"In a small, sleepy town nestled between rolling hills, there was an old, neglected music shop called “Harmonies Lost.” Its windows were cracked, the wooden sign swayed in the breeze, and dust covered every surface inside. Most of the townsfolk had forgotten about it, passing it by as they hurried on with their busy lives. But to one person, it was a place of wonder—a place that held the key to a mystery she couldn’t let go of.\"\n",
    "\n",
    "## encode the text to utf-8\n",
    "tokens = sample_text.encode('utf-8')\n",
    "print(list(tokens))  # Display the byte values as a list\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{128, 148, 153, 156, 157, 32, 44, 46, 66, 72, 73, 76, 77, 97, 98, 99, 100, 101, 226, 103, 104, 105, 102, 107, 108, 109, 110, 111, 112, 114, 115, 116, 117, 118, 119, 121, 122}\n",
      "226 32\n"
     ]
    }
   ],
   "source": [
    "print(set(tokens))\n",
    "print(max(tokens), min(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((104, 101), 175), ((101, 32), 175), ((116, 104), 139), ((32, 116), 136), ((100, 32), 113), ((32, 115), 111), ((116, 32), 106), ((44, 32), 96), ((32, 97), 94), ((115, 32), 86), ((105, 110), 84), ((101, 114), 78), ((32, 119), 76), ((101, 100), 74), ((110, 32), 72), ((32, 104), 72), ((97, 110), 66), ((110, 103), 62), ((46, 32), 60), ((226, 128), 53), ((111, 110), 53), ((114, 32), 52), ((101, 110), 51), ((32, 98), 48), ((32, 105), 48), ((97, 115), 46), ((114, 101), 45), ((105, 116), 45), ((104, 97), 44), ((32, 102), 44), ((109, 101), 41), ((97, 32), 40), ((115, 116), 40), ((103, 32), 40), ((119, 97), 40), ((111, 32), 39), ((32, 111), 38), ((97, 116), 38), ((110, 100), 37), ((115, 104), 36), ((32, 112), 36), ((110, 101), 35), ((104, 105), 35), ((116, 101), 34), ((32, 99), 34), ((111, 117), 34), ((32, 108), 34), ((111, 114), 33), ((101, 101), 31), ((121, 32), 31), ((115, 111), 31), ((116, 111), 30), ((118, 101), 30), ((101, 97), 30), ((108, 111), 29), ((32, 100), 29), ((101, 108), 28), ((108, 105), 28), ((97, 114), 28), ((110, 116), 28), ((111, 109), 28), ((110, 111), 28), ((10, 10), 27), ((98, 101), 27), ((32, 109), 27), ((108, 97), 26), ((108, 101), 25), ((101, 116), 25), ((104, 32), 25), ((105, 108), 24), ((102, 111), 24), ((115, 101), 24), ((84, 104), 23), ((115, 105), 23), ((32, 110), 22), ((101, 115), 21), ((114, 111), 21), ((103, 104), 21), ((111, 116), 20), ((107, 101), 20), ((117, 116), 20), ((105, 97), 20), ((117, 115), 19), ((116, 44), 19), ((108, 100), 18), ((119, 105), 18), ((111, 111), 18), ((32, 101), 18), ((111, 102), 18), ((102, 32), 18), ((97, 100), 18), ((97, 108), 17), ((100, 101), 17), ((128, 153), 17), ((111, 100), 16), ((108, 108), 16), ((32, 114), 16), ((97, 99), 16), ((99, 101), 16), ((114, 105), 16)]\n"
     ]
    }
   ],
   "source": [
    "## ex : To compress the given text , \n",
    "\n",
    "# first step find byte pairs.\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    " \n",
    "stats = get_stats(tokens)\n",
    "#print(stats)\n",
    "sorted_stats = list(stats.items())\n",
    "print(sorted(sorted_stats, key=lambda item: item[1], reverse=True)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  t\n",
      "e  \n",
      "n  \n"
     ]
    }
   ],
   "source": [
    "## lets find out the most frequent pairs, mostly which are followed by spaces.\n",
    "print(chr(32), chr(116))\n",
    "print(chr(101), chr(32))\n",
    "print(chr(110), chr(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (104, 101) into a new token 256\n",
      "merging (32, 116) into a new token 257\n",
      "merging (256, 32) into a new token 258\n",
      "merging (32, 115) into a new token 259\n",
      "merging (32, 97) into a new token 260\n",
      "merging (105, 110) into a new token 261\n",
      "merging (116, 32) into a new token 262\n",
      "merging (101, 100) into a new token 263\n",
      "merging (257, 258) into a new token 264\n",
      "merging (46, 32) into a new token 265\n",
      "merging (115, 32) into a new token 266\n",
      "merging (44, 32) into a new token 267\n",
      "merging (226, 128) into a new token 268\n",
      "merging (111, 110) into a new token 269\n",
      "merging (101, 114) into a new token 270\n",
      "merging (101, 110) into a new token 271\n",
      "merging (104, 97) into a new token 272\n",
      "merging (261, 103) into a new token 273\n",
      "merging (101, 32) into a new token 274\n",
      "merging (119, 97) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "#### **BPE implementation**\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "# ---\n",
    "vocab_size = 276 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256 ## our unique tokens are 226, for our sample text.\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "    \n",
    "# print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets try with a larger text, and observe the token size and compression ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "**The Forgotten Melody**\n",
      "\n",
      "In a small, sleepy town nestled between rolling hills, there was an old, neglected music shop called “Harmonies Lost.” Its windows were cracked, the wooden sign swayed in the \n",
      "\n",
      "length: 5791\n",
      "---\n",
      "[42, 42, 84, 104, 101, 32, 70, 111, 114, 103, 111, 116, 116, 101, 110, 32, 77, 101, 108, 111, 100, 121, 42, 42, 10, 10, 73, 110, 32, 97, 32, 115, 109, 97, 108, 108, 44, 32, 115, 108, 101, 101, 112, 121, 32, 116, 111, 119, 110, 32, 110, 101, 115, 116, 108, 101, 100, 32, 98, 101, 116, 119, 101, 101, 110, 32, 114, 111, 108, 108, 105, 110, 103, 32, 104, 105, 108, 108, 115, 44, 32, 116, 104, 101, 114, 101, 32, 119, 97, 115, 32, 97, 110, 32, 111, 108, 100, 44, 32, 110]\n",
      "length: 5897\n",
      "--------------------------------\n",
      "tokens length: 5897\n",
      "ids length: 1506\n",
      "compression ratio: 3.92X\n"
     ]
    }
   ],
   "source": [
    "## now lets try with a larger text.\n",
    "with open('sample_1000_words.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 just for convenience\n",
    "print('---')\n",
    "print(text[:200], '\\n')\n",
    "print(\"length:\", len(text))\n",
    "print('---')\n",
    "print(tokens[:100]) ## print first 100 tokens\n",
    "print(\"length:\", len(tokens))\n",
    "print('--------------------------------')\n",
    "\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "# ---\n",
    "vocab_size = 1000 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256 ## our unique tokens are 226, for our sample text.\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    if not stats:  # Check if stats is empty to avoid errors\n",
    "        break\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "\n",
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
