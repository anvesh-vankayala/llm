{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets do tokenization in non-english language Telugu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before encoding: తెలుగు భాష ఒక ద్రావిడ భాష. 26\n",
      "after encoding: [224, 176, 164, 224, 177, 134, 224, 176, 178, 224, 177, 129, 224, 176, 151, 224, 177, 129, 32, 224, 176, 173, 224, 176, 190, 224, 176, 183, 32, 224, 176, 146, 224, 176, 149, 32, 224, 176, 166, 224, 177, 141, 224, 176, 176, 224, 176, 190, 224, 176, 181, 224, 176, 191, 224, 176, 161, 32, 224, 176, 173, 224, 176, 190, 224, 176, 183, 46]\n",
      "length of tokens: 68\n"
     ]
    }
   ],
   "source": [
    "## write me a code to tokenize the text in telugu, with BPE in python.\n",
    "sample_text = \"తెలుగు భాష ఒక ద్రావిడ భాష.\"\n",
    "print('before encoding:', sample_text, len(sample_text))\n",
    "tokens = sample_text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "print('after encoding:', tokens)\n",
    "print('length of tokens:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "telugu_text befores utf-8 encoding:  సుశీలమ్మ కళ్ళలో భయం పారాడింది. అనాధ బిడ్డ అని చిన్నప్పుడే తెలిస్తే మన దగ్గిరవాడు అలా అరమరిక లేకుండా\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV files\n",
    "file_paths = [\n",
    "    '/Users/anvesh/codebase/llm/data/telugu_books/telugu_books.csv',\n",
    "    '/Users/anvesh/codebase/llm/data/telugu_news/1_telugu_news.csv',\n",
    "    '/Users/anvesh/codebase/llm/data/telugu_news/2_telugu_news.csv'\n",
    "]\n",
    "\n",
    "# Combine data from all files\n",
    "telugu_texts = []\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'text' in df.columns:\n",
    "        telugu_texts.append(' '.join(df['text'].astype(str).tolist()))\n",
    "    elif 'body' in df.columns:\n",
    "        telugu_texts.append(' '.join(df['body'].astype(str).tolist()))\n",
    "\n",
    "# Concatenate all texts and remove all English, numerical values, and quotes\n",
    "telugu_text = ' '.join(telugu_texts)\n",
    "telugu_text = re.sub(r'[A-Za-z0-9\\'\"]', '', telugu_text)  # Remove English letters, numbers, and quotes\n",
    "telugu_text = re.sub(r'[\\r\\n\\xa0]', '', telugu_text)  # Remove line breaks and non-breaking spaces\n",
    "\n",
    "print('telugu_text befores utf-8 encoding:', telugu_text[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text size: 143512307\n",
      "Vocabulary size of telugu_text: 1985864\n",
      "Original text size: 143512307\n",
      "Unique character count in telugu_text: 194\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(set(telugu_text.split()))\n",
    "print('Original text size:', len(telugu_text))\n",
    "print('Vocabulary size of telugu_text:', vocabulary_size)\n",
    "\n",
    "unique_characters = set(telugu_text)\n",
    "unique_count = len(unique_characters)\n",
    "print('Original text size:', len(telugu_text))\n",
    "print('Unique character count in telugu_text:', unique_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|██████████| 144/144 [00:08<00:00, 17.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_tokens: [b' ', b'\\xe0\\xb0\\xb8', b'\\xe0\\xb1\\x81', b'\\xe0\\xb0\\xb6', b'\\xe0\\xb1\\x80', b'\\xe0\\xb0\\xb2', b'\\xe0\\xb0\\xae', b'\\xe0\\xb1\\x8d', b'\\xe0\\xb0\\xae', b' ', b'\\xe0\\xb0\\x95', b'\\xe0\\xb0\\xb3', b'\\xe0\\xb1\\x8d', b'\\xe0\\xb0\\xb3', b'\\xe0\\xb0\\xb2', b'\\xe0\\xb1\\x8b', b' ', b'\\xe0\\xb0\\xad', b'\\xe0\\xb0\\xaf', b'\\xe0\\xb0\\x82', b' ', b'\\xe0\\xb0\\xaa', b'\\xe0\\xb0\\xbe', b'\\xe0\\xb0\\xb0', b'\\xe0\\xb0\\xbe', b'\\xe0\\xb0\\xa1', b'\\xe0\\xb0\\xbf', b'\\xe0\\xb0\\x82', b'\\xe0\\xb0\\xa6', b'\\xe0\\xb0\\xbf', b'.', b' ', b'\\xe0\\xb0\\x85', b'\\xe0\\xb0\\xa8', b'\\xe0\\xb0\\xbe', b'\\xe0\\xb0\\xa7', b' ', b'\\xe0\\xb0\\xac', b'\\xe0\\xb0\\xbf', b'\\xe0\\xb0\\xa1', b'\\xe0\\xb1\\x8d', b'\\xe0\\xb0\\xa1', b' ', b'\\xe0\\xb0\\x85', b'\\xe0\\xb0\\xa8', b'\\xe0\\xb0\\xbf', b' ', b'\\xe0\\xb0\\x9a', b'\\xe0\\xb0\\xbf', b'\\xe0\\xb0\\xa8', b'\\xe0\\xb1\\x8d', b'\\xe0\\xb0\\xa8', b'\\xe0\\xb0\\xaa', b'\\xe0\\xb1\\x8d', b'\\xe0\\xb0\\xaa', b'\\xe0\\xb1\\x81', b'\\xe0\\xb0\\xa1', b'\\xe0\\xb1\\x87', b' ', b'\\xe0\\xb0\\xa4', b'\\xe0\\xb1\\x86', b'\\xe0\\xb0\\xb2', b'\\xe0\\xb0\\xbf', b'\\xe0\\xb0\\xb8', b'\\xe0\\xb1\\x8d', b'\\xe0\\xb0\\xa4', b'\\xe0\\xb1\\x87', b' ', b'\\xe0\\xb0\\xae', b'\\xe0\\xb0\\xa8', b' ', b'\\xe0\\xb0\\xa6', b'\\xe0\\xb0\\x97', b'\\xe0\\xb1\\x8d', b'\\xe0\\xb0\\x97', b'\\xe0\\xb0\\xbf', b'\\xe0\\xb0\\xb0', b'\\xe0\\xb0\\xb5', b'\\xe0\\xb0\\xbe', b'\\xe0\\xb0\\xa1', b'\\xe0\\xb1\\x81', b' ', b'\\xe0\\xb0\\x85', b'\\xe0\\xb0\\xb2', b'\\xe0\\xb0\\xbe', b' ', b'\\xe0\\xb0\\x85', b'\\xe0\\xb0\\xb0', b'\\xe0\\xb0\\xae', b'\\xe0\\xb0\\xb0', b'\\xe0\\xb0\\xbf', b'\\xe0\\xb0\\x95', b' ', b'\\xe0\\xb0\\xb2', b'\\xe0\\xb1\\x87', b'\\xe0\\xb0\\x95', b'\\xe0\\xb1\\x81', b'\\xe0\\xb0\\x82', b'\\xe0\\xb0\\xa1', b'\\xe0\\xb0\\xbe']\n",
      "143512307\n",
      "Time taken to encode and process tokens in parallel: 14.8965 seconds\n"
     ]
    }
   ],
   "source": [
    "import utils.encode_parallel_telugu as encode_parallel\n",
    "import time\n",
    "\n",
    "tokens = encode_parallel.load_telugu_texts()\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "# Encode the tokens in parallel and get concatenated results\n",
    "encoded_tokens = encode_parallel.encode_tokens_parallel(tokens, chunk_size=1_000_000, max_workers=10)\n",
    "print('encoded_tokens:', encoded_tokens[:100])\n",
    "print(len(encoded_tokens))\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to encode and process tokens in parallel: {end_time - start_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length of encoded_text:', len(encoded_tokens))\n",
    "print('unique characters in encoded_text:', set(encoded_tokens))\n",
    "print('unique characters in encoded_text:', len(set(encoded_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tokens[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging tokens: 100%|██████████| 244/244 [8:39:11<00:00, 127.67s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 143512307\n",
      "ids length: 77428527\n",
      "compression ratio: 1.85X\n",
      "token size: 194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ## lets read with bigger text\n",
    "# with open('sample_telugu.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "# tokens = telugu_text.encode(\"utf-8\")\n",
    "#### **BPE implementation**\n",
    "\n",
    "tokens = encoded_tokens\n",
    "\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "# ---\n",
    "vocab_size = 500 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256 ## our unique tokens are 194, for our sample text.\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "for i in tqdm(range(num_merges), desc=\"Merging tokens\"):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    # print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx # merge has a pair of tokens and the new token index\n",
    "    \n",
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")\n",
    "print(f\"token size: {len(set(tokens))}\")\n",
    "    \n",
    "# print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(i,end=' ') for i in set(ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143512307"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## lets read with bigger text\n",
    "# with open('sample_telugu.txt', 'r') as f:\n",
    "#     text = f.read()\n",
    "# tokens = telugu_text.encode(\"utf-8\")\n",
    "#### **BPE implementation**\n",
    "\n",
    "tokens = encoded_tokens\n",
    "\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "# ---\n",
    "vocab_size = 500 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256 ## our unique tokens are 194, for our sample text.\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "for i in tqdm(range(num_merges), desc=\"Merging tokens\"):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    # print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx # merge has a pair of tokens and the new token index\n",
    "    \n",
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")\n",
    "print(f\"token size: {len(set(tokens))}\")\n",
    "    \n",
    "# print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number in millions is: 143.512307\n"
     ]
    }
   ],
   "source": [
    "millions = 143512307 / 1_000_000\n",
    "print(f\"The number in millions is: {millions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('merges.json', 'w') as f:\n",
    "        json.dump(merges, f)  # Save merges separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "telugu_unicode_chars = [chr(i) for i in range(0x0C00, 0x0C7F)]  # Telugu Unicode range\n",
    "\n",
    "# Add these characters to the vocabulary\n",
    "import json\n",
    "\n",
    "vocab = {token: idx for token, idx in merges.items()}\n",
    "\n",
    "\n",
    "\n",
    "# Add unique Telugu characters to the vocabulary\n",
    "for idx, char in enumerate([chr(i).encode('utf-8') for i in range(0x0C00, 0x0C7F)]):\n",
    "    if idx < 256:  # Ensure we only add up to 256 characters\n",
    "        vocab[char] = idx  # Map the character to its index\n",
    "\n",
    "vocab[b' '] = 255\n",
    "vocab[b'.'] = 254\n",
    "# Save merges and vocab to a file\n",
    "# with open('merges_vocab.json', 'w') as f:\n",
    "#     json.dump({'merges': merges, 'vocab': vocab}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('merges_vocab.json', 'w') as f:\n",
    "    json.dump({'merges': {str(k): v for k, v in merges.items()}, 'vocab': {str(k): v for k, v in vocab.items()}}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Read the merges and vocab data from the JSON file\n",
    "with open('merges_vocab.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a defaultdict to store the data in a distributed manner\n",
    "distributed_data = defaultdict(list)\n",
    "\n",
    "# Distribute the merges and vocab data\n",
    "# for key, value in data['merges'].items():\n",
    "#     distributed_data['merges'].append({key: value})\n",
    "\n",
    "for key, value in data['vocab'].items():\n",
    "    distributed_data['vocab'].append({key: value})\n",
    "\n",
    "# Optionally, print the distributed data for verification\n",
    "print(distributed_data)\n",
    "distributed_data['vocab']\n",
    "# Convert the list of dictionaries to a single dictionary\n",
    "formatted_vocab = {}\n",
    "for item in distributed_data['vocab']:\n",
    "    for k, v in item.items():\n",
    "        if ',' not in k:\n",
    "            formatted_vocab[(eval(k),)] = v\n",
    "        else:\n",
    "            formatted_vocab[eval(k)] = v\n",
    "print(formatted_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{256: (b'\\xe0\\xb0\\xbf', b' '),\n",
       " 257: (b'\\xe0\\xb1\\x81', b' '),\n",
       " 258: (b'.', b' '),\n",
       " 259: (b'\\xe0\\xb0\\xa8', b'\\xe0\\xb1\\x8d'),\n",
       " 260: (b'\\xe0\\xb0\\xbe', b' '),\n",
       " 261: (b'\\xe0\\xb0\\x82', b'\\xe0\\xb0\\xa6'),\n",
       " 262: (259, b'\\xe0\\xb0\\xa8'),\n",
       " 263: (b'\\xe0\\xb0\\xb0', b'\\xe0\\xb1\\x8d'),\n",
       " 264: (b'\\xe0\\xb1\\x8d', b'\\xe0\\xb0\\xb0'),\n",
       " 265: (b'\\xe0\\xb0\\xb8', b'\\xe0\\xb1\\x8d'),\n",
       " 266: (b'\\xe0\\xb1\\x8b', b' '),\n",
       " 267: (b'\\xe0\\xb1\\x81', 258),\n",
       " 268: (b'\\xe0\\xb0\\x82', b' '),\n",
       " 269: (b'\\xe0\\xb0\\xbe', b'\\xe0\\xb0\\xb0'),\n",
       " 270: (b'\\xe0\\xb0\\xa8', b'\\xe0\\xb0\\xbf'),\n",
       " 271: (b'\\xe0\\xb1\\x87', b' '),\n",
       " 272: (b'\\xe0\\xb1\\x81', b'\\xe0\\xb0\\x95'),\n",
       " 273: (b'\\xe0\\xb0\\xbe', b'\\xe0\\xb0\\xb2'),\n",
       " 274: (b'\\xe0\\xb0\\xa8', 256),\n",
       " 275: (b'\\xe0\\xb0\\x82', b'\\xe0\\xb0\\x9a'),\n",
       " 276: (b'\\xe0\\xb0\\x9f', b'\\xe0\\xb1\\x8d'),\n",
       " 277: (b'\\xe0\\xb0\\xbf', 258),\n",
       " 278: (b'\\xe0\\xb0\\x95', b'\\xe0\\xb1\\x8d'),\n",
       " 279: (b'\\xe0\\xb0\\xbe', b'\\xe0\\xb0\\xa1'),\n",
       " 280: (b' ', b'\\xe0\\xb0\\x85'),\n",
       " 281: (b'\\xe0\\xb1\\x81', b'\\xe0\\xb0\\xb2'),\n",
       " 282: (b'.', b'.'),\n",
       " 283: (b'\\xe0\\xb0\\x82', b'\\xe0\\xb0\\x9f'),\n",
       " 284: (b'\\xe0\\xb0\\x97', 260),\n",
       " 285: (b' ', b'\\xe0\\xb0\\xaa'),\n",
       " 286: (b'\\xe0\\xb1\\x8d', b'\\xe0\\xb0\\xaf'),\n",
       " 287: (b'\\xe0\\xb0\\xb0', b'\\xe0\\xb0\\xbf'),\n",
       " 288: (b'\\xe0\\xb1\\x81', 262),\n",
       " 289: (b' ', b'\\xe0\\xb0\\xb5'),\n",
       " 290: (b' ', b'\\xe0\\xb0\\xae'),\n",
       " 291: (b'\\xe0\\xb0\\xaa', b'\\xe0\\xb1\\x8d'),\n",
       " 292: (b' ', b'\\xe0\\xb0\\x95'),\n",
       " 293: (265, b'\\xe0\\xb0\\xa4'),\n",
       " 294: (b'\\xe0\\xb0\\xbf', b'\\xe0\\xb0\\xa8'),\n",
       " 295: (b'\\xe0\\xb0\\x82', b'\\xe0\\xb0\\xa4'),\n",
       " 296: (261, 277),\n",
       " 297: (b'\\xe0\\xb0\\x82', b'\\xe0\\xb0\\xa1'),\n",
       " 298: (b'\\xe0\\xb0\\xb2', 266),\n",
       " 299: (b'\\xe0\\xb0\\x95', 256),\n",
       " 300: (b'\\xe0\\xb0\\xbe', b'\\xe0\\xb0\\xa8'),\n",
       " 301: (b'\\xe0\\xb0\\xb2', b'\\xe0\\xb1\\x8d'),\n",
       " 302: (b'\\xe0\\xb0\\x9a', b'\\xe0\\xb1\\x87'),\n",
       " 303: (b' ', b' '),\n",
       " 304: (291, b'\\xe0\\xb0\\xaa'),\n",
       " 305: (b'\\xe0\\xb0\\xbf', 275),\n",
       " 306: (b'\\xe0\\xb0\\xb3', b'\\xe0\\xb1\\x8d'),\n",
       " 307: (276, b'\\xe0\\xb0\\x9f'),\n",
       " 308: (b',', b' '),\n",
       " 309: (b'\\xe0\\xb0\\xb0', b'\\xe0\\xb0\\xbe'),\n",
       " 310: (306, b'\\xe0\\xb0\\xb3'),\n",
       " 311: (b'\\xe0\\xb0\\xb2', b'\\xe0\\xb1\\x87'),\n",
       " 312: (b'\\xe0\\xb0\\xaa', b'\\xe0\\xb1\\x8b'),\n",
       " 313: (b'\\xe0\\xb0\\xbe', b'\\xe0\\xb0\\xaf'),\n",
       " 314: (b'\\xe0\\xb0\\xa8', 257),\n",
       " 315: (b'\\xe0\\xb0\\x9a', b'\\xe0\\xb1\\x8d'),\n",
       " 316: (b'\\xe0\\xb0\\xb5', b'\\xe0\\xb0\\xbf'),\n",
       " 317: (b'\\xe0\\xb1\\x81', b'\\xe0\\xb0\\xa4'),\n",
       " 318: (b'\\xe0\\xb0\\x95', 257),\n",
       " 319: (b' ', b'\\xe0\\xb0\\xb8'),\n",
       " 320: (315, b'\\xe0\\xb0\\x9a'),\n",
       " 321: (b'\\xe0\\xb0\\xae', b'\\xe0\\xb0\\xbe'),\n",
       " 322: (278, b'\\xe0\\xb0\\x95'),\n",
       " 323: (279, 267),\n",
       " 324: (b'\\xe0\\xb0\\xb2', 257),\n",
       " 325: (b'\\xe0\\xb1\\x81', b'\\xe0\\xb0\\xa1'),\n",
       " 326: (b'\\xe0\\xb0\\xaa', 264),\n",
       " 327: (b'\\xe0\\xb0\\xb2', b'\\xe0\\xb0\\xbf'),\n",
       " 328: (b'\\xe0\\xb0\\xb5', b'\\xe0\\xb0\\xbe'),\n",
       " 329: (b'\\xe0\\xb0\\xa4', b'\\xe0\\xb1\\x8d'),\n",
       " 330: (b' ', b'\\xe0\\xb0\\xa8'),\n",
       " 331: (b'\\xe0\\xb0\\x95', b'\\xe0\\xb0\\xbe'),\n",
       " 332: (b'\\xe0\\xb0\\xa6', b'\\xe0\\xb1\\x8d'),\n",
       " 333: (b'\\xe0\\xb0\\x9a', b'\\xe0\\xb1\\x86'),\n",
       " 334: (b' ', b'\\xe0\\xb0\\xa4'),\n",
       " 335: (b'\\xe0\\xb0\\xbf', 296),\n",
       " 336: (b'\\xe0\\xb0\\xb2', b'\\xe0\\xb1\\x8b'),\n",
       " 337: (272, b'\\xe0\\xb1\\x81'),\n",
       " 338: (b'\\xe0\\xb0\\xae', b'\\xe0\\xb1\\x8d'),\n",
       " 339: (b'\\xe0\\xb0\\xbf', b'\\xe0\\xb0\\xa4'),\n",
       " 340: (b'\\xe0\\xb0\\x82', b'\\xe0\\xb0\\x97'),\n",
       " 341: (b'\\xe0\\xb1\\x80', b' '),\n",
       " 342: (269, 267),\n",
       " 343: (b'\\xe0\\xb0\\x82', 284),\n",
       " 344: (b'\\xe0\\xb0\\x9a', b'\\xe0\\xb1\\x82'),\n",
       " 345: (b'\\xe0\\xb1\\x81', b'\\xe0\\xb0\\xb5'),\n",
       " 346: (b'\\xe0\\xb0\\xa1', b'\\xe0\\xb0\\xbf'),\n",
       " 347: (b'\\xe0\\xb0\\xa4', b'\\xe0\\xb0\\xbf'),\n",
       " 348: (b'\\xe0\\xb1\\x81', b'\\xe0\\xb0\\x97'),\n",
       " 349: (b'\\xe0\\xb1\\x8d', b'\\xe0\\xb0\\xb5'),\n",
       " 350: (261, 256),\n",
       " 351: (b' ', b'\\xe0\\xb0\\x86'),\n",
       " 352: (b'\\xe0\\xb0\\xae', b'\\xe0\\xb1\\x86'),\n",
       " 353: (b'\\xe0\\xb0\\xa6', b'\\xe0\\xb0\\xbf'),\n",
       " 354: (b'\\xe0\\xb0\\xae', b'\\xe0\\xb1\\x80'),\n",
       " 355: (b'\\xe0\\xb0\\xaa', b'\\xe0\\xb1\\x86'),\n",
       " 356: (b'\\xe0\\xb0\\xb5', b'\\xe0\\xb1\\x86'),\n",
       " 357: (b'\\xe0\\xb0\\xa6', b'\\xe0\\xb0\\xbe'),\n",
       " 358: (b'\\xe0\\xb0\\xae', b'\\xe0\\xb1\\x81'),\n",
       " 359: (279, 257),\n",
       " 360: (b'\\xe0\\xb0\\xb8', b'\\xe0\\xb0\\xbf'),\n",
       " 361: (b'\\xe0\\xb0\\xb7', b'\\xe0\\xb1\\x8d'),\n",
       " 362: (b'\\xe0\\xb0\\xb5', b'\\xe0\\xb1\\x81'),\n",
       " 363: (b'\\xe0\\xb0\\xa8', b'\\xe0\\xb1\\x87'),\n",
       " 364: (301, b'\\xe0\\xb0\\xb2'),\n",
       " 365: (b'\\xe0\\xb0\\x9f', b'\\xe0\\xb0\\xbf'),\n",
       " 366: (b'\\xe0\\xb0\\x95', b'\\xe0\\xb1\\x82'),\n",
       " 367: (312, b'\\xe0\\xb0\\xaf'),\n",
       " 368: (b'\\xe0\\xb0\\xa4', b'\\xe0\\xb1\\x86'),\n",
       " 369: (311, b'\\xe0\\xb0\\xa6'),\n",
       " 370: (303, 303),\n",
       " 371: (b'\\xe0\\xb0\\xb2', b'\\xe0\\xb0\\xbe'),\n",
       " 372: (b'\\xe0\\xb0\\xa8', b'\\xe0\\xb0\\xbe'),\n",
       " 373: (b'\\xe0\\xb0\\xb5', b'\\xe0\\xb1\\x87'),\n",
       " 374: (338, b'\\xe0\\xb0\\xae'),\n",
       " 375: (278, b'\\xe0\\xb0\\xb7'),\n",
       " 376: (b'\\xe0\\xb0\\xb2', b'\\xe0\\xb1\\x81'),\n",
       " 377: (285, 264),\n",
       " 378: (332, b'\\xe0\\xb0\\xa6'),\n",
       " 379: (b'\\xe0\\xb0\\xa8', b'\\xe0\\xb1\\x81'),\n",
       " 380: (b'\\xe0\\xb0\\xbe', 263),\n",
       " 381: (b'\\xe0\\xb0\\x97', b'\\xe0\\xb1\\x81'),\n",
       " 382: (270, 299),\n",
       " 383: (281, 257),\n",
       " 384: (b'\\xe0\\xb0\\xae', b'\\xe0\\xb0\\xbf'),\n",
       " 385: (333, 304),\n",
       " 386: (b'\\xe0\\xb0\\xa8', 271),\n",
       " 387: (b'\\xe0\\xb0\\xac', b'\\xe0\\xb0\\xbe'),\n",
       " 388: (b'\\xe0\\xb0\\xa8', b'\\xe0\\xb1\\x80'),\n",
       " 389: (b'\\xe0\\xb0\\x95', b'\\xe0\\xb1\\x8a'),\n",
       " 390: (b'\\xe0\\xb0\\x95', b'\\xe0\\xb1\\x81'),\n",
       " 391: (b'\\xe0\\xb0\\xa6', 256),\n",
       " 392: (262, 256),\n",
       " 393: (b'\\xe0\\xb0\\xa1', b'\\xe0\\xb1\\x8d'),\n",
       " 394: (302, b'\\xe0\\xb0\\xb8'),\n",
       " 395: (b'\\xe0\\xb0\\xb0', b'\\xe0\\xb1\\x81'),\n",
       " 396: (329, b'\\xe0\\xb0\\xa4'),\n",
       " 397: (b'\\xe0\\xb0\\xb2', 260),\n",
       " 398: (b'\\xe0\\xb0\\x97', b'\\xe0\\xb1\\x8d'),\n",
       " 399: (275, 256),\n",
       " 400: (b'\\xe0\\xb0\\x9a', b'\\xe0\\xb0\\xbf'),\n",
       " 401: (b'\\xe0\\xb0\\xa6', b'\\xe0\\xb1\\x87'),\n",
       " 402: (b'\\xe0\\xb0\\xb0', 257),\n",
       " 403: (b'\\xe0\\xb0\\xaf', 286),\n",
       " 404: (289, b'\\xe0\\xb0\\xbf'),\n",
       " 405: (b'\\xe0\\xb0\\xa4', 264),\n",
       " 406: (b'\\xe0\\xb0\\x82', 298),\n",
       " 407: (b'\\xe0\\xb0\\x85', b'\\xe0\\xb0\\xa4'),\n",
       " 408: (b'\\xe0\\xb1\\x82', b' '),\n",
       " 409: (b'\\xe0\\xb0\\xaf', b'\\xe0\\xb0\\xbf'),\n",
       " 410: (b'\\xe0\\xb0\\xbe', b'\\xe0\\xb0\\xb5'),\n",
       " 411: (304, 325),\n",
       " 412: (b'\\xe0\\xb0\\xa8', b' '),\n",
       " 413: (272, b'\\xe0\\xb1\\x8b'),\n",
       " 414: (b'\\xe0\\xb0\\xaa', b'\\xe0\\xb0\\xbf'),\n",
       " 415: (282, 258),\n",
       " 416: (b'\\xe0\\xb0\\xb5', 320),\n",
       " 417: (b'\\xe0\\xb0\\xb0', b'\\xe0\\xb1\\x8b'),\n",
       " 418: (293, 288),\n",
       " 419: (257, b'\\xe0\\xb0\\x85'),\n",
       " 420: (b'\\xe0\\xb0\\x92', b'\\xe0\\xb0\\x95'),\n",
       " 421: (282, b'.'),\n",
       " 422: (b'\\xe0\\xb0\\x97', b'\\xe0\\xb0\\xbf'),\n",
       " 423: (b'\\xe0\\xb0\\x82', b'\\xe0\\xb0\\x95'),\n",
       " 424: (b'\\xe0\\xb1\\x80', b'\\xe0\\xb0\\xb8'),\n",
       " 425: (b'\\xe0\\xb0\\xa8', 260),\n",
       " 426: (b'\\xe0\\xb0\\xb0', b'\\xe0\\xb1\\x86'),\n",
       " 427: (283, 271),\n",
       " 428: (b' ', b'\\xe0\\xb0\\x87'),\n",
       " 429: (b'\\xe0\\xb0\\xac', b'\\xe0\\xb1\\x8d'),\n",
       " 430: (273, 257),\n",
       " 431: (b'\\xe0\\xb0\\xb5', b'\\xe0\\xb0\\xb0'),\n",
       " 432: (b'\\xe0\\xb0\\xa4', 266),\n",
       " 433: (272, 288),\n",
       " 434: (b'\\xe0\\xb0\\xaa', b'\\xe0\\xb0\\xbe'),\n",
       " 435: (b'\\xe2\\x80\\x8c', b' '),\n",
       " 436: (b'\\xe0\\xb0\\xa4', 256),\n",
       " 437: (261, 272),\n",
       " 438: (b'\\xe0\\xb0\\xbe', 382),\n",
       " 439: (b'\\xe0\\xb0\\xa1', 260),\n",
       " 440: (b'\\xe0\\xb0\\xaa', b'\\xe0\\xb0\\xa1'),\n",
       " 441: (b'\\xe0\\xb0\\xb8', b'\\xe0\\xb0\\xbe'),\n",
       " 442: (b'\\xe0\\xb0\\xbf', b'\\xe0\\xb0\\x95'),\n",
       " 443: (276, b'\\xe0\\xb0\\xb2'),\n",
       " 444: (261, b'\\xe0\\xb0\\xbf'),\n",
       " 445: (b'\\xe0\\xb0\\x86', 352),\n",
       " 446: (369, 267),\n",
       " 447: (361, b'\\xe0\\xb0\\x9f'),\n",
       " 448: (b'\\xe0\\xb0\\xaa', 257),\n",
       " 449: (b'\\xe0\\xb0\\x95', b'\\xe0\\xb1\\x8b'),\n",
       " 450: (273, b'\\xe0\\xb1\\x8d'),\n",
       " 451: (b'\\xe0\\xb0\\xa4', b'\\xe0\\xb0\\xa8'),\n",
       " 452: (b'\\xe0\\xb0\\xa6', b'\\xe0\\xb1\\x81'),\n",
       " 453: (300, 267),\n",
       " 454: (b' ', b'\\xe0\\xb0\\x97'),\n",
       " 455: (256, b'\\xe0\\xb0\\x85'),\n",
       " 456: (b' ', b'\\xe0\\xb0\\x8e'),\n",
       " 457: (b' ', b'\\xe0\\xb0\\xac'),\n",
       " 458: (313, 277),\n",
       " 459: (b'\\xe0\\xb0\\xae', b'\\xe0\\xb1\\x88'),\n",
       " 460: (263, b'\\xe0\\xb0\\x9a'),\n",
       " 461: (b'\\xe0\\xb0\\xaa', 305),\n",
       " 462: (286, b'\\xe0\\xb0\\xbe'),\n",
       " 463: (265, b'\\xe0\\xb0\\x9f'),\n",
       " 464: (312, b'\\xe0\\xb0\\xa4'),\n",
       " 465: (b'\\xe0\\xb0\\x95', b'\\xe0\\xb0\\xbf'),\n",
       " 466: (b'\\xe0\\xb0\\xb0', b'\\xe0\\xb1\\x82'),\n",
       " 467: (b'\\xe0\\xb0\\xb5', 269),\n",
       " 468: (302, b'\\xe0\\xb0\\xaf'),\n",
       " 469: (317, 288),\n",
       " 470: (b'\\xe0\\xb0\\xb5', 349),\n",
       " 471: (b'\\xe0\\xb0\\xb8', 256),\n",
       " 472: (b'\\xe0\\xb0\\xb0', b'\\xe0\\xb1\\x87'),\n",
       " 473: (b'\\xe0\\xb0\\x82', 258),\n",
       " 474: (b'\\xe0\\xb0\\xaf', b'\\xe0\\xb0\\xbe'),\n",
       " 475: (b'\\xe0\\xb0\\xa1', 268),\n",
       " 476: (337, 283),\n",
       " 477: (265, b'\\xe0\\xb0\\xa5'),\n",
       " 478: (b'\\xe0\\xb0\\xb8', b'\\xe0\\xb1\\x81'),\n",
       " 479: (b' ', b'\\xe0\\xb0\\x9a'),\n",
       " 480: (b'\\xe0\\xb0\\xaf', 268),\n",
       " 481: (295, 266),\n",
       " 482: (356, 310),\n",
       " 483: (b'?', 280),\n",
       " 484: (b'\\xe0\\xb0\\x85', 262),\n",
       " 485: (b'\\xe0\\xb0\\xb6', b'\\xe0\\xb1\\x8d'),\n",
       " 486: (b'\\xe0\\xb0\\xae', b'\\xe0\\xb0\\xa8'),\n",
       " 487: (b'\\xe0\\xb0\\x9c', b'\\xe0\\xb1\\x80'),\n",
       " 488: (322, b'\\xe0\\xb0\\xa1'),\n",
       " 489: (355, 307),\n",
       " 490: (b'\\xe0\\xb0\\xaa', b'\\xe0\\xb1\\x88'),\n",
       " 491: (398, b'\\xe0\\xb0\\x97'),\n",
       " 492: (b'\\xe0\\xb0\\xa6', 267),\n",
       " 493: (281, b'\\xe0\\xb1\\x8d'),\n",
       " 494: (354, b'\\xe0\\xb0\\xa6'),\n",
       " 495: (393, b'\\xe0\\xb0\\xa1'),\n",
       " 496: (b'!', b' '),\n",
       " 497: (b'\\xe0\\xb1\\x82', 263),\n",
       " 498: (294, b' '),\n",
       " 499: (b'\\xe0\\xb1\\x81', 296),\n",
       " 0: (b'\\xe0\\xb0\\x80',),\n",
       " 1: (b'\\xe0\\xb0\\x81',),\n",
       " 2: (b'\\xe0\\xb0\\x82',),\n",
       " 3: (b'\\xe0\\xb0\\x83',),\n",
       " 4: (b'\\xe0\\xb0\\x84',),\n",
       " 5: (b'\\xe0\\xb0\\x85',),\n",
       " 6: (b'\\xe0\\xb0\\x86',),\n",
       " 7: (b'\\xe0\\xb0\\x87',),\n",
       " 8: (b'\\xe0\\xb0\\x88',),\n",
       " 9: (b'\\xe0\\xb0\\x89',),\n",
       " 10: (b'\\xe0\\xb0\\x8a',),\n",
       " 11: (b'\\xe0\\xb0\\x8b',),\n",
       " 12: (b'\\xe0\\xb0\\x8c',),\n",
       " 13: (b'\\xe0\\xb0\\x8d',),\n",
       " 14: (b'\\xe0\\xb0\\x8e',),\n",
       " 15: (b'\\xe0\\xb0\\x8f',),\n",
       " 16: (b'\\xe0\\xb0\\x90',),\n",
       " 17: (b'\\xe0\\xb0\\x91',),\n",
       " 18: (b'\\xe0\\xb0\\x92',),\n",
       " 19: (b'\\xe0\\xb0\\x93',),\n",
       " 20: (b'\\xe0\\xb0\\x94',),\n",
       " 21: (b'\\xe0\\xb0\\x95',),\n",
       " 22: (b'\\xe0\\xb0\\x96',),\n",
       " 23: (b'\\xe0\\xb0\\x97',),\n",
       " 24: (b'\\xe0\\xb0\\x98',),\n",
       " 25: (b'\\xe0\\xb0\\x99',),\n",
       " 26: (b'\\xe0\\xb0\\x9a',),\n",
       " 27: (b'\\xe0\\xb0\\x9b',),\n",
       " 28: (b'\\xe0\\xb0\\x9c',),\n",
       " 29: (b'\\xe0\\xb0\\x9d',),\n",
       " 30: (b'\\xe0\\xb0\\x9e',),\n",
       " 31: (b'\\xe0\\xb0\\x9f',),\n",
       " 32: (b'\\xe0\\xb0\\xa0',),\n",
       " 33: (b'\\xe0\\xb0\\xa1',),\n",
       " 34: (b'\\xe0\\xb0\\xa2',),\n",
       " 35: (b'\\xe0\\xb0\\xa3',),\n",
       " 36: (b'\\xe0\\xb0\\xa4',),\n",
       " 37: (b'\\xe0\\xb0\\xa5',),\n",
       " 38: (b'\\xe0\\xb0\\xa6',),\n",
       " 39: (b'\\xe0\\xb0\\xa7',),\n",
       " 40: (b'\\xe0\\xb0\\xa8',),\n",
       " 41: (b'\\xe0\\xb0\\xa9',),\n",
       " 42: (b'\\xe0\\xb0\\xaa',),\n",
       " 43: (b'\\xe0\\xb0\\xab',),\n",
       " 44: (b'\\xe0\\xb0\\xac',),\n",
       " 45: (b'\\xe0\\xb0\\xad',),\n",
       " 46: (b'\\xe0\\xb0\\xae',),\n",
       " 47: (b'\\xe0\\xb0\\xaf',),\n",
       " 48: (b'\\xe0\\xb0\\xb0',),\n",
       " 49: (b'\\xe0\\xb0\\xb1',),\n",
       " 50: (b'\\xe0\\xb0\\xb2',),\n",
       " 51: (b'\\xe0\\xb0\\xb3',),\n",
       " 52: (b'\\xe0\\xb0\\xb4',),\n",
       " 53: (b'\\xe0\\xb0\\xb5',),\n",
       " 54: (b'\\xe0\\xb0\\xb6',),\n",
       " 55: (b'\\xe0\\xb0\\xb7',),\n",
       " 56: (b'\\xe0\\xb0\\xb8',),\n",
       " 57: (b'\\xe0\\xb0\\xb9',),\n",
       " 58: (b'\\xe0\\xb0\\xba',),\n",
       " 59: (b'\\xe0\\xb0\\xbb',),\n",
       " 60: (b'\\xe0\\xb0\\xbc',),\n",
       " 61: (b'\\xe0\\xb0\\xbd',),\n",
       " 62: (b'\\xe0\\xb0\\xbe',),\n",
       " 63: (b'\\xe0\\xb0\\xbf',),\n",
       " 64: (b'\\xe0\\xb1\\x80',),\n",
       " 65: (b'\\xe0\\xb1\\x81',),\n",
       " 66: (b'\\xe0\\xb1\\x82',),\n",
       " 67: (b'\\xe0\\xb1\\x83',),\n",
       " 68: (b'\\xe0\\xb1\\x84',),\n",
       " 69: (b'\\xe0\\xb1\\x85',),\n",
       " 70: (b'\\xe0\\xb1\\x86',),\n",
       " 71: (b'\\xe0\\xb1\\x87',),\n",
       " 72: (b'\\xe0\\xb1\\x88',),\n",
       " 73: (b'\\xe0\\xb1\\x89',),\n",
       " 74: (b'\\xe0\\xb1\\x8a',),\n",
       " 75: (b'\\xe0\\xb1\\x8b',),\n",
       " 76: (b'\\xe0\\xb1\\x8c',),\n",
       " 77: (b'\\xe0\\xb1\\x8d',),\n",
       " 78: (b'\\xe0\\xb1\\x8e',),\n",
       " 79: (b'\\xe0\\xb1\\x8f',),\n",
       " 80: (b'\\xe0\\xb1\\x90',),\n",
       " 81: (b'\\xe0\\xb1\\x91',),\n",
       " 82: (b'\\xe0\\xb1\\x92',),\n",
       " 83: (b'\\xe0\\xb1\\x93',),\n",
       " 84: (b'\\xe0\\xb1\\x94',),\n",
       " 85: (b'\\xe0\\xb1\\x95',),\n",
       " 86: (b'\\xe0\\xb1\\x96',),\n",
       " 87: (b'\\xe0\\xb1\\x97',),\n",
       " 88: (b'\\xe0\\xb1\\x98',),\n",
       " 89: (b'\\xe0\\xb1\\x99',),\n",
       " 90: (b'\\xe0\\xb1\\x9a',),\n",
       " 91: (b'\\xe0\\xb1\\x9b',),\n",
       " 92: (b'\\xe0\\xb1\\x9c',),\n",
       " 93: (b'\\xe0\\xb1\\x9d',),\n",
       " 94: (b'\\xe0\\xb1\\x9e',),\n",
       " 95: (b'\\xe0\\xb1\\x9f',),\n",
       " 96: (b'\\xe0\\xb1\\xa0',),\n",
       " 97: (b'\\xe0\\xb1\\xa1',),\n",
       " 98: (b'\\xe0\\xb1\\xa2',),\n",
       " 99: (b'\\xe0\\xb1\\xa3',),\n",
       " 100: (b'\\xe0\\xb1\\xa4',),\n",
       " 101: (b'\\xe0\\xb1\\xa5',),\n",
       " 102: (b'\\xe0\\xb1\\xa6',),\n",
       " 103: (b'\\xe0\\xb1\\xa7',),\n",
       " 104: (b'\\xe0\\xb1\\xa8',),\n",
       " 105: (b'\\xe0\\xb1\\xa9',),\n",
       " 106: (b'\\xe0\\xb1\\xaa',),\n",
       " 107: (b'\\xe0\\xb1\\xab',),\n",
       " 108: (b'\\xe0\\xb1\\xac',),\n",
       " 109: (b'\\xe0\\xb1\\xad',),\n",
       " 110: (b'\\xe0\\xb1\\xae',),\n",
       " 111: (b'\\xe0\\xb1\\xaf',),\n",
       " 112: (b'\\xe0\\xb1\\xb0',),\n",
       " 113: (b'\\xe0\\xb1\\xb1',),\n",
       " 114: (b'\\xe0\\xb1\\xb2',),\n",
       " 115: (b'\\xe0\\xb1\\xb3',),\n",
       " 116: (b'\\xe0\\xb1\\xb4',),\n",
       " 117: (b'\\xe0\\xb1\\xb5',),\n",
       " 118: (b'\\xe0\\xb1\\xb6',),\n",
       " 119: (b'\\xe0\\xb1\\xb7',),\n",
       " 120: (b'\\xe0\\xb1\\xb8',),\n",
       " 121: (b'\\xe0\\xb1\\xb9',),\n",
       " 122: (b'\\xe0\\xb1\\xba',),\n",
       " 123: (b'\\xe0\\xb1\\xbb',),\n",
       " 124: (b'\\xe0\\xb1\\xbc',),\n",
       " 125: (b'\\xe0\\xb1\\xbd',),\n",
       " 126: (b'\\xe0\\xb1\\xbe',),\n",
       " 255: (b' ',),\n",
       " 254: (b'.',)}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_vocab = {v: k for k, v in formatted_vocab.items()}\n",
    "inverted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bytes(value):\n",
    "    if isinstance(value, bytes):\n",
    "        return value\n",
    "    elif value in inverted_vocab:\n",
    "        return process_tuple(inverted_vocab[value])\n",
    "    else:\n",
    "        print(f'value not found in inverted_vocab: {value}')\n",
    "        return None\n",
    "\n",
    "def process_tuple(value_tuple):\n",
    "    # print(f'value_tuple: {value_tuple}')\n",
    "    # for vi in value_tuple:\n",
    "    #     print(f'v: {vi}')\n",
    "    converted_values = []\n",
    "    for v in value_tuple:\n",
    "        result = convert_to_bytes(v)\n",
    "        if isinstance(result, tuple):\n",
    "            converted_values.extend(result)\n",
    "        else:\n",
    "            converted_values.append(result)\n",
    "    return tuple(converted_values)\n",
    "\n",
    "decoder_map = {k: process_tuple(v) for k, v in inverted_vocab.items()}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "త ె ల ు గ ు   భ ా ష   ఒ క   ద ్ ర ా వ ి డ   -   భ ా ష . \n",
      "\n",
      "36 70 50 65 23 65 255 45 62 55 255 18 21 255 38 77 48 62 53 63 33 255 255 45 62 55 254 \n",
      "\n",
      "b'-' \n",
      "\n",
      "తెలుగు భాష ఒక ద్రావిడ  భాష.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"తెలుగు భాష ఒక ద్రావిడ భాష.\"\n",
    "li = encode_parallel.encode_tokens_parallel(text, chunk_size=1_000_000, max_workers=10)\n",
    "_ = [print(i.decode('utf-8'), end=' ') for i in li]\n",
    "print('\\n')\n",
    "_ = [print(vocab[i], end=' ') for i in li if i in vocab]\n",
    "print('\\n')\n",
    "_ = [print(i, end=' ') for i in li if i not in vocab]\n",
    "encoed_li  = [vocab[i] for i in li if i in vocab]\n",
    "print('\\n')\n",
    "decoded_text = ''.join([k.decode('utf-8') for i in encoed_li for k, v in vocab.items() if v == i])\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"తెలుగు భాష ఒక ద్రావిడ భాష.\"\n",
    "# li = encode_parallel.encode_tokens_parallel(text, chunk_size=1_000_000, max_workers=10)\n",
    "# # [print(i.decode('utf-8')) for i in li]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, mapping):\n",
    "    encoded = []\n",
    "    # Convert the text to bytes\n",
    "    byte_text = encode_parallel.encode_tokens_parallel(text, chunk_size=1_000_000, max_workers=10)\n",
    "    print(f'byte_text: {byte_text}')\n",
    "    print(f'mapping: {mapping}')\n",
    "    # Iterate through the byte pairs in the text\n",
    "    i = 0\n",
    "    while i < len(byte_text):\n",
    "        # Check for pairs\n",
    "        if i < len(byte_text) - 1:\n",
    "            pair = (byte_text[i:i+2], byte_text[i+1:i+2])\n",
    "            # pair = tuple(pair)\n",
    "            print(f'pair: {pair}')\n",
    "            if pair in mapping:\n",
    "                print(f'pair: {pair}')\n",
    "                encoded.append(mapping[pair])\n",
    "                i += 2  # Move past the pair\n",
    "                continue\n",
    "        \n",
    "        # Check for single byte matches\n",
    "        single_byte = byte_text[i:i+1]\n",
    "        if single_byte in mapping:\n",
    "            encoded.append(mapping[single_byte])\n",
    "        else:\n",
    "            # If no match, append the original byte as an index (or handle as needed)\n",
    "            encoded.append(single_byte)\n",
    "        \n",
    "        i += 1  # Move to the next byte\n",
    "\n",
    "    return encoded\n",
    "\n",
    "# Example usage\n",
    "text_to_encode = \"తెలుగు భాష ఒక ద్రావిడ భాష.\"  # Replace with your actual text\n",
    "encoded_output = encode_text(text_to_encode, vocab)\n",
    "print(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"తెలుగు భాష ఒక ద్రావిడ భాష.\"\n",
    "print(text)\n",
    "\n",
    "byte_text = encode_parallel.encode_tokens_parallel(text, chunk_size=1_000_000, max_workers=10)\n",
    "i = 0\n",
    "while i < len(byte_text):\n",
    "    if i < len(byte_text) - 1:\n",
    "        te = []\n",
    "        for i,val in enumerate(byte_text[i:i+2]):\n",
    "            te.append(val)\n",
    "        for i,val in enumerate(byte_text[i+1:i+2]):\n",
    "            te.append(val)\n",
    "        pair = tuple(te)\n",
    "        print(f'pair: {pair}')\n",
    "        # if pair in vocab:\n",
    "        #     encoded.append(vocab[pair])\n",
    "        i += 2  # Move past the pair\n",
    "# (byte_text[i:i+2], byte_text[i+1:i+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe0\\xb1\\x86'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't concat int to bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m byte_text[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: can't concat int to bytes"
     ]
    }
   ],
   "source": [
    "byte_text[i+1:i+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_list = [item for sublist in li for item in sublist]\n",
    "print(unpacked_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97, 98, 99, 32, 100]\n"
     ]
    }
   ],
   "source": [
    "tokens = 'abc d'.encode('utf-8')\n",
    "tokens = list(map(int, tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ఀ\n",
      "తెలుగు\n"
     ]
    }
   ],
   "source": [
    "print(b'\\xe0\\xb0\\x80'.decode('utf-8'))\n",
    "print(b'\\xe0\\xb0\\xa4\\xe0\\xb1\\x86\\xe0\\xb0\\xb2\\xe0\\xb1\\x81\\xe0\\xb0\\x97\\xe0\\xb1\\x81'.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m li \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa4\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb1\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\x86\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb2\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb1\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\x81\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\x97\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb1\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\x81\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xad\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xbe\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb7\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\x92\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\x95\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa6\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb1\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\x8d\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xbe\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb5\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xbf\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa1\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xad\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xbe\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xe0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xb7\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# for i in li:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(li\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "li = [b'\\xe0', b'\\xb0', b'\\xa4', b'\\xe0', b'\\xb1', b'\\x86', b'\\xe0', b'\\xb0', b'\\xb2', b'\\xe0', b'\\xb1', b'\\x81', b'\\xe0', b'\\xb0', b'\\x97', b'\\xe0', b'\\xb1', b'\\x81', b' ', b'\\xe0', b'\\xb0', b'\\xad', b'\\xe0', b'\\xb0', b'\\xbe', b'\\xe0', b'\\xb0', b'\\xb7', b' ', b'\\xe0', b'\\xb0', b'\\x92', b'\\xe0', b'\\xb0', b'\\x95', b' ', b'\\xe0', b'\\xb0', b'\\xa6', b'\\xe0', b'\\xb1', b'\\x8d', b'\\xe0', b'\\xb0', b'\\xb0', b'\\xe0', b'\\xb0', b'\\xbe', b'\\xe0', b'\\xb0', b'\\xb5', b'\\xe0', b'\\xb0', b'\\xbf', b'\\xe0', b'\\xb0', b'\\xa1', b' ', b'\\xe0', b'\\xb0', b'\\xad', b'\\xe0', b'\\xb0', b'\\xbe', b'\\xe0', b'\\xb0', b'\\xb7', b'.']\n",
    "# for i in li:\n",
    "print(li.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "తెలుగు భాష ఒక ద్రావిడ భాష.\n"
     ]
    }
   ],
   "source": [
    "decoded_text = b''.join([b'\\xe0', b'\\xb0', b'\\xa4', b'\\xe0', b'\\xb1', b'\\x86', b'\\xe0', b'\\xb0', b'\\xb2', b'\\xe0', b'\\xb1', b'\\x81', b'\\xe0', b'\\xb0', b'\\x97', b'\\xe0', b'\\xb1', b'\\x81', b' ', b'\\xe0', b'\\xb0', b'\\xad', b'\\xe0', b'\\xb0', b'\\xbe', b'\\xe0', b'\\xb0', b'\\xb7', b' ', b'\\xe0', b'\\xb0', b'\\x92', b'\\xe0', b'\\xb0', b'\\x95', b' ', b'\\xe0', b'\\xb0', b'\\xa6', b'\\xe0', b'\\xb1', b'\\x8d', b'\\xe0', b'\\xb0', b'\\xb0', b'\\xe0', b'\\xb0', b'\\xbe', b'\\xe0', b'\\xb0', b'\\xb5', b'\\xe0', b'\\xb0', b'\\xbf', b'\\xe0', b'\\xb0', b'\\xa1', b' ', b'\\xe0', b'\\xb0', b'\\xad', b'\\xe0', b'\\xb0', b'\\xbe', b'\\xe0', b'\\xb0', b'\\xb7', b'.'])\n",
    "decoded_text = decoded_text.decode('utf-8')\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe0\\xb0\\xa4\\xe0\\xb1\\x86\\xe0\\xb0\\xb2\\xe0\\xb1\\x81\\xe0\\xb0\\x97\\xe0\\xb1\\x81 \\xe0\\xb0\\xad\\xe0\\xb0\\xbe\\xe0\\xb0\\xb7 \\xe0\\xb0\\x92\\xe0\\xb0\\x95 \\xe0\\xb0\\xa6\\xe0\\xb1\\x8d\\xe0\\xb0\\xb0\\xe0\\xb0\\xbe\\xe0\\xb0\\xb5\\xe0\\xb0\\xbf\\xe0\\xb0\\xa1 \\xe0\\xb0\\xad\\xe0\\xb0\\xbe\\xe0\\xb0\\xb7.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b''.join([b'\\xe0', b'\\xb0', b'\\xa4', b'\\xe0', b'\\xb1', b'\\x86', b'\\xe0', b'\\xb0', b'\\xb2', b'\\xe0', b'\\xb1', b'\\x81', b'\\xe0', b'\\xb0', b'\\x97', b'\\xe0', b'\\xb1', b'\\x81', b' ', b'\\xe0', b'\\xb0', b'\\xad', b'\\xe0', b'\\xb0', b'\\xbe', b'\\xe0', b'\\xb0', b'\\xb7', b' ', b'\\xe0', b'\\xb0', b'\\x92', b'\\xe0', b'\\xb0', b'\\x95', b' ', b'\\xe0', b'\\xb0', b'\\xa6', b'\\xe0', b'\\xb1', b'\\x8d', b'\\xe0', b'\\xb0', b'\\xb0', b'\\xe0', b'\\xb0', b'\\xbe', b'\\xe0', b'\\xb0', b'\\xb5', b'\\xe0', b'\\xb0', b'\\xbf', b'\\xe0', b'\\xb0', b'\\xa1', b' ', b'\\xe0', b'\\xb0', b'\\xad', b'\\xe0', b'\\xb0', b'\\xbe', b'\\xe0', b'\\xb0', b'\\xb7', b'.'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byte_text: b'\\xe0\\xb0\\xa4\\xe0\\xb1\\x86\\xe0\\xb0\\xb2\\xe0\\xb1\\x81\\xe0\\xb0\\x97\\xe0\\xb1\\x81 \\xe0\\xb0\\xad\\xe0\\xb0\\xbe\\xe0\\xb0\\xb7 \\xe0\\xb0\\x92\\xe0\\xb0\\x95 \\xe0\\xb0\\xa6\\xe0\\xb1\\x8d\\xe0\\xb0\\xb0\\xe0\\xb0\\xbe\\xe0\\xb0\\xb5\\xe0\\xb0\\xbf\\xe0\\xb0\\xa1 \\xe0\\xb0\\xad\\xe0\\xb0\\xbe\\xe0\\xb0\\xb7.'\n",
      "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "def encode_text(text, mapping):\n",
    "    encoded = []\n",
    "    # Convert the text to bytes\n",
    "    byte_text = text.encode('utf-8')\n",
    "    print(f'byte_text: {byte_text}')\n",
    "    # Iterate through the byte pairs in the text\n",
    "    i = 0\n",
    "    while i < len(byte_text):\n",
    "        # Check for pairs\n",
    "        if i < len(byte_text) - 1:\n",
    "            pair = (byte_text[i:i+1], byte_text[i+1:i+1])  # Adjusted to match the mapping structure\n",
    "            if pair in mapping:\n",
    "                encoded.append(mapping[pair])\n",
    "                i += 2  # Move past the pair\n",
    "                continue\n",
    "        \n",
    "        # Check for single byte matches\n",
    "        single_byte = byte_text[i:i+1]\n",
    "        if single_byte in mapping:\n",
    "            encoded.append(mapping[single_byte])\n",
    "        else:\n",
    "            # If no match, append a placeholder or handle as needed\n",
    "            encoded.append(-1)  # Placeholder for unmatched bytes\n",
    "        \n",
    "        i += 1  # Move to the next byte\n",
    "\n",
    "    return encoded\n",
    "\n",
    "# Example usage\n",
    "text_to_encode = \"తెలుగు భాష ఒక ద్రావిడ భాష.\"  # Replace with your actual text\n",
    "encoded_output = encode_text(text_to_encode, vocab)\n",
    "print(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now lets try with a larger text.\n",
    "with open('sample_1000_words.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 just for convenience\n",
    "print('---')\n",
    "print(text[:200], '\\n')\n",
    "print(\"length:\", len(text))\n",
    "print('---')\n",
    "print(tokens[:100]) ## print first 100 tokens\n",
    "print(\"length:\", len(tokens))\n",
    "print('--------------------------------')\n",
    "\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "# ---\n",
    "vocab_size = 1000 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256 ## our unique tokens are 226, for our sample text.\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    if not stats:  # Check if stats is empty to avoid errors\n",
    "        break\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "\n",
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
